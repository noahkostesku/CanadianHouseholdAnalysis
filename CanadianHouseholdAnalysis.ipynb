{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will start by importing all requied libraries needed for this project.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn.feature_extraction.text as sktext\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculations\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "import shap.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Data</h3>\n",
    "<p>Loading in both of our datasets from local files.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_stats_df = pl.read_csv(\"Coursework Data/DemoStats.csv\", null_values=None)\n",
    "household_spend_df = pl.read_csv(\"Coursework Data/HouseholdSpend.csv\", null_values=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Merging Data</h3>\n",
    "<p>Next, we will combine these two datasets into one. We joined them on the \"CODE\" column, which is the common column between the two datasets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = demo_stats_df.join(\n",
    "    household_spend_df,\n",
    "    on=[\"CODE\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Drop ID columns\n",
    "merged_df = merged_df.drop([\"GEO\", \"CODE\"])\n",
    "\n",
    "# Describe the data\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The data is now merged. One thing that stands out to me is the extreme values at each end of the distribution. We will need to take a look at these values and see if they are valid or if they are outliers. We will do this later.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1: Clustering and Dimensionality Reduction</h1>\n",
    "\n",
    "The first part of the coursework will focus\n",
    "on identifying the characteristics of Canadian households, excluding their pension\n",
    "behaviour. For this, do not include, in your clustering and dimensionality reduction models,\n",
    "the target of the regression model in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target variable\n",
    "merged_df_clust = merged_df.drop(\"HSEP001S\")\n",
    "merged_df_clust = merged_df_clust.drop(\"HSHNIAGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Null Values</h3>\n",
    "<p>We will check for any null values in our dataset. If there are any, we will need to decide on how to handle them.</p>\n",
    "<p>From doing a quick scan of the database, it seems null values are listed as string of \"NA\"s. We will need to convert these to actual null values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_points = merged_df_clust.height\n",
    "print(f\"Total data points: {total_data_points}\")\n",
    "# Define a function to convert NA to None\n",
    "def convert_na_to_nulls(df):\n",
    "    return df.with_columns([\n",
    "        pl.col(col).replace(\"NA\", None).alias(col)\n",
    "        if df.schema[col] == pl.Utf8 else pl.col(col)\n",
    "        for col in df.columns\n",
    "    ])\n",
    "\n",
    "# Convert NA to None on both dataframes\n",
    "merged_df_clust = convert_na_to_nulls(merged_df_clust)\n",
    "\n",
    "# Get total nulls for each column\n",
    "def count_na_strings(df):\n",
    "    total_rows = df.height\n",
    "    return {\n",
    "        col: {\n",
    "            \"null_count\": df[col].null_count(),\n",
    "            \"less than 1%\": 1 / total_rows < 0.01,\n",
    "        }\n",
    "        for col in df.columns\n",
    "        if df[col].null_count() > 0\n",
    "    }\n",
    "\n",
    "merged_df_nulls = count_na_strings(merged_df_clust)\n",
    "print(f\"Data Null Count: {merged_df_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to turn any string into a number\n",
    "def convert_strings_to_numbers(df):\n",
    "    if hasattr(df, \"to_pandas\"):\n",
    "        df = df.to_pandas()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in [\"CODE\", \"GEO\"]:\n",
    "            continue  # skip explicitly excluded columns\n",
    "\n",
    "        if df[col].dtype == \"object\" or pd.api.types.is_string_dtype(df[col]):\n",
    "            if (df[col] == \"NA\").any():\n",
    "                continue  # skip if \"NA\" appears in the column\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError:\n",
    "                df[col] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "    return pl.from_pandas(df)\n",
    "\n",
    "# Convert df\n",
    "merged_df_clust = convert_strings_to_numbers(merged_df_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Wow, we have a lot of null values in our dataset. However, after a quick inspection, it seems a lot of these null values are soley from rows that contain only zero. Here is an example...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display entire row\n",
    "def display_row(df, row_index):\n",
    "    df = df.to_pandas()\n",
    "    with pd.option_context(\n",
    "        'display.max_columns', None,\n",
    "        'display.max_colwidth', None,\n",
    "        'display.width', None,\n",
    "        'display.expand_frame_repr', False\n",
    "    ):\n",
    "        row = df.iloc[row_index]\n",
    "        print(row.to_string())\n",
    "    return row\n",
    "\n",
    "row = display_row(merged_df_clust, 461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Handle Nulls</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>That being said, we will need to remove these rows from our dataset. We will do this by removing any rows that contain only zeroes and nulls.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with only nulls or zeros\n",
    "def remove_rows_with_nulls_or_zeros(df):\n",
    "    df = df.to_pandas()\n",
    "    # This mask is True for rows where all **non-null** values are 0\n",
    "    mask = df.apply(lambda row: (row.dropna() == 0).all(), axis=1)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    return pl.from_pandas(df)\n",
    "\n",
    "merged_df_clust = remove_rows_with_nulls_or_zeros(merged_df_clust)\n",
    "# Check for any remaining nulls\n",
    "merged_df_nulls = count_na_strings(merged_df_clust)\n",
    "print(f\"Data Null Count: {merged_df_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>That definitely helped. We still have some nulls values but not as many. For the rest, we can substitute them with the median of the column. This is a common practice in data science and will help us keep our dataset clean.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_nulls_with_median(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Convert to Pandas\n",
    "    df_pd = df.to_pandas()\n",
    "\n",
    "    for col in df_pd.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_pd[col]):\n",
    "            if df_pd[col].isnull().sum() > 0:\n",
    "                median = df_pd[col].median()\n",
    "                print(f\"Filling nulls in '{col}' with median = {median}\")\n",
    "                df_pd[col] = df_pd[col].fillna(median)\n",
    "\n",
    "    # Back to Polars\n",
    "    return pl.from_pandas(df_pd)\n",
    "\n",
    "\n",
    "merged_df_clust = substitute_nulls_with_median(merged_df_clust)\n",
    "\n",
    "merged_df_nulls = count_na_strings(merged_df_clust)\n",
    "print(f\"Data Null Count: {merged_df_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Negative Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total negative values for each column\n",
    "def count_negative_values(df):\n",
    "    return {\n",
    "        col: {\n",
    "            \"negative_count\": df[col].filter(df[col] < 0).len(),\n",
    "        }\n",
    "        for col in df.columns\n",
    "        if df.schema[col] in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                              pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                              pl.Float32, pl.Float64]\n",
    "        and df[col].filter(df[col] < 0).len() > 0\n",
    "    }\n",
    "\n",
    "# Get total negative values for each dataframe\n",
    "merged_df_negatives = count_negative_values(merged_df_clust)\n",
    "print(f\"Data Negative Values Count: {merged_df_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see that there are negative values in the dataset:<p>\n",
    "<ul>\n",
    "<li>HSTT001 - Total expenditure,Household Expenditures (Category Summary),Dollars</li>\n",
    "<li>HSTE001ZBS - Total non-current consumption,Household Expenditures (Category Summary),Dollars</li>\n",
    "<li>HSWH040S,Net purchase price of owned residences,Household Expenditures (Category Summary),Dollars</li>\n",
    "<li>HSWH041S - Net purchase price of owned secondary residences,Household Expenditures (Category Summary),Dollars</li>\n",
    "<li>HSWH042S - Net purchase price of other owned properties,Household Expenditures (Category Summary),Dollars</li>\n",
    "</ul>\n",
    "<p>We can easily tell that the first two varaibles cannot be negative, since they desribe expenditures, and because they capture sums of outflows. The next 3 are tricky, because these variables reflect the net purchase price of owned residences, secondary residences, and other properties, they can indeed be negative if the proceeds from selling those properties exceed any purchase or improvement costs, thereby indicating a net inflow rather than an outflow.</p>\n",
    "<p>Therefore, we will remove the negatives in the first two variables, and keep the last three.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_negatives(df, cols):\n",
    "    updated_df = df.clone()\n",
    "\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            neg_count = df.select((pl.col(col) < 0).sum()).item()\n",
    "\n",
    "            if neg_count > 0:\n",
    "                print(f\"Removing {neg_count} rows with negative values in '{col}'\")\n",
    "                updated_df = updated_df.filter(pl.col(col) >= 0)\n",
    "    return updated_df\n",
    "\n",
    "# Replace negative values with median\n",
    "merged_df_clust = remove_negatives(merged_df_clust, [\"HSTT001\", \"HSTE001ZBS\"])\n",
    "merged_df_negatives = count_negative_values(merged_df_clust)\n",
    "print(f\"Data Negative Values Count: {merged_df_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>General Clean Up</h3>\n",
    "<p>Here, we will be cleaning up any data that is redundant, such as rows with straight zeros, columns where the mean and std are both zero, implying that the column is constant, and any other data that is not useful.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(merged_df_clust):\n",
    "    # Drop any rows that contain straight 0's in all columns\n",
    "    columns_to_check = [col for col in merged_df_clust.columns if merged_df_clust.schema[col] in (pl.Int64, pl.Float64)]\n",
    "    merged_df_clust = merged_df_clust.filter(\n",
    "        ~pl.all_horizontal([pl.col(col) == 0 for col in columns_to_check])\n",
    "    )\n",
    "\n",
    "    # Drop any columns where the mean and std are both 0\n",
    "    columns_to_drop = [\n",
    "        col for col in merged_df_clust.columns\n",
    "        if merged_df_clust.schema[col] in (pl.Int64, pl.Float64)\n",
    "        and merged_df_clust[col].mean() == 0\n",
    "        and merged_df_clust[col].std() == 0\n",
    "    ]\n",
    "    merged_df_clust = merged_df_clust.drop(columns_to_drop)\n",
    "    return merged_df_clust\n",
    "\n",
    "merged_df_clust = clean_up(merged_df_clust)\n",
    "\n",
    "merged_df_clust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Correlated Variables</h3>\n",
    "<p>Now, we will be looking for correlated variables. We will be using the correlation matrix to find the correlation between the variables.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Start by removing anything with the word income, retirement, pension, insurance, premium\n",
    "except for target column calculation variable</p>\n",
    "<p>We do this because we are trying to predict the target variable, which is the Total personal insurance premiums and retirement/pension contributions. We want to remove any variables that are correlated with the target variable, so that we can get a better prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_related_target_columns(merged_df_clust):\n",
    "    # Drop columns that are related to the target variable\n",
    "    columns_to_drop = [\n",
    "        \"HSEP001\", \"HSSH006\", \"HSSH014\", \"HSSH019\", \"HSSH044\", \"HSEP002\",\n",
    "        \"HSEP003\", \"HSEP004\", \"HSEP005\", \"HSEP006\", \"HSEP007\", \"HSEP008\",\n",
    "        \"HSEP009\", \"HSHC022\", \"HSHC023\", \"HSHC024\", \"HSHC025\", \"HSTR025\",\n",
    "        \"HSRV011\", \"HSAGDISCIN\", \"HSAGDISPIN\"\n",
    "    ]\n",
    "    for col in columns_to_drop:\n",
    "        if col in merged_df_clust.columns:\n",
    "            merged_df_clust = merged_df_clust.drop(col)\n",
    "            \n",
    "    return merged_df_clust\n",
    "\n",
    "\n",
    "merged_df_clust = drop_related_target_columns(merged_df_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, lets create a function to remove perfectly correlated variables. We will be using the correlation matrix to find the correlation between the variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_perfectly_correlated_vars(df):\n",
    "    numeric_df = df.select(pl.selectors.numeric())\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    columns = numeric_df.columns\n",
    "\n",
    "    correlated_pairs = {\n",
    "        \"var_1\": [],\n",
    "        \"var_2\": [],\n",
    "        \"correlation\": []\n",
    "    }\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            corr_value = corr_matrix.select(columns[j]).row(i)[0]\n",
    "            if corr_value == 1.0 or corr_value == -1.0:\n",
    "                correlated_pairs[\"var_1\"].append(columns[i])\n",
    "                correlated_pairs[\"var_2\"].append(columns[j])\n",
    "                correlated_pairs[\"correlation\"].append(corr_value)\n",
    "\n",
    "    return pl.DataFrame(correlated_pairs).sort(\"correlation\", descending=True)\n",
    "\n",
    "\n",
    "# Get highly correlated variables\n",
    "df_correlated = summarize_perfectly_correlated_vars(merged_df_clust)\n",
    "print(\"Df Highly Correlated Variables:\", df_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see that certain variables are highly correlated with other variables, this is because we have a large number of aggregate variables that are derived from the same underlying data. To fix this, we will drop variables with a correlation count of over 1, since these can be determined as having redundancy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_perfectly_correlated_columns(df: pl.DataFrame) -> tuple[pl.DataFrame, list[str]]:\n",
    "    numeric_df = df.select(pl.selectors.numeric())\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    columns = numeric_df.columns\n",
    "    to_drop = set()\n",
    "    already_seen = set()\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        col_i = columns[i]\n",
    "        if col_i in to_drop:\n",
    "            continue\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_j = columns[j]\n",
    "            if col_j in to_drop:\n",
    "                continue\n",
    "            corr_val = corr_matrix.select(col_j).row(i)[0]\n",
    "            if abs(corr_val) == 1.0 and col_j not in already_seen:\n",
    "                to_drop.add(col_j)\n",
    "        already_seen.add(col_i)\n",
    "\n",
    "    cleaned_df = df.drop(to_drop)\n",
    "    return cleaned_df, sorted(to_drop)\n",
    "\n",
    "\n",
    "merged_df_clust, dropped_columns = remove_perfectly_correlated_columns(merged_df_clust)\n",
    "print(f\"Dropped {len(dropped_columns)} highly correlated columns:\")\n",
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Outliers</h3>\n",
    "<p>One major issue with this data is how skewed it is. The data is heavily skewed to the right, with a very long tail. For example, we are getting values of over 20,000, tailing all the way down to 100, but then the mean of that column will be around 40.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, column, xlim=None, numbins=300):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column], bins=numbins, kde=True)\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(merged_df_clust, \"ECYBASPOP\", xlim=(0, 1000), numbins=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can clearly see how skewed the data is. The mean is around 40, but the median is around 20. This is a clear indication that the data is skewed to the right. We can also see that the data has a long tail that goes past 1000 (it actually goes to 20,000).</p>\n",
    "<p>To fix this, we will first use a power transform to make the data more normal.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_power_transformer(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Convert to Pandas\n",
    "    df_pd = df.to_pandas()\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df_pd.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Apply PowerTransformer to each numeric column\n",
    "    transformer = PowerTransformer(standardize=False)\n",
    "    df_pd[numeric_cols] = transformer.fit_transform(df_pd[numeric_cols])\n",
    "\n",
    "    # Convert back to Polars\n",
    "    return pl.from_pandas(df_pd)\n",
    "\n",
    "cleaned_df = apply_power_transformer(merged_df_clust)\n",
    "\n",
    "plot_distribution(cleaned_df, \"ECYBASPOP\", xlim=(0, 20), numbins=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Width: {cleaned_df.width}\")\n",
    "print(f\"Height: {cleaned_df.height}\")\n",
    "\n",
    "cleaned_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see our data is left with 936 columns and just over 600,000 rows.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Clustering</h2>\n",
    "<p>We will be using the KMeans clustering algorithm to cluster the data. We will be using the elbow method, in contrast with the silhouette method to find the optimal number of clusters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling</h3>\n",
    "<p>Scaling will be completed using the RobustScaler, which is a scaler that is robust to outliers. This will help us to scale the data without being affected by the outliers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler \n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Convert to Pandas DataFrame for scaling\n",
    "cleaned_df = cleaned_df.to_pandas()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(cleaned_df)\n",
    "\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(cleaned_df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=cleaned_df.columns)\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a sample of the data</h3>\n",
    "<p>We will create a sample of the data, so that we can run our clustering models faster.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20% of the data\n",
    "sampled_df = scaled_df.sample(frac=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_stats = scaled_df.mean()[:5]\n",
    "sample_stats = sampled_df.mean()[:5]\n",
    "\n",
    "print(\"Full Data Means:\\n\", full_stats)\n",
    "print(\"\\nSample Data Means:\\n\", sample_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Means Clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a K-Means clustering of the data, identifying the optimal number of\n",
    "clusters using both the silhouette and the elbow method.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Start by using the elbow method to identify the optimal number of clusters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KClusterer\n",
    "KClusterer = KMeans(n_clusters=3,\n",
    "                    verbose=0,\n",
    "                    random_state=2025)\n",
    "# Use KElbowVisualizer to find optimal number of clusters\n",
    "visualizer = KElbowVisualizer(KClusterer, # Cluster model with any parameters you need\n",
    "                              k=(2,16),   # Number of clusters to test (2 to 12 in this case)\n",
    "                              locate_elbow=True, # Locate the elbow? Default is true.\n",
    "                              timings=False # Plot the timings to train?\n",
    "                             )\n",
    "\n",
    "visualizer.fit(sampled_df)       # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see the optimal number of clusters is 5, since the knee of the curve is at 5. This is a weak elbow, so we can confirm by using the silouette method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Analysis\n",
    "def plot_silhouette(data, n_clusters):\n",
    "    # Initialize the clusterer\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=2025)\n",
    "    cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "    silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "    sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          alpha=0.7)\n",
    "        plt.text(-0.1, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "    plt.title(f\"Silhouette plot for {n_clusters} clusters\")\n",
    "    plt.xlabel(\"Silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "# Run silhouette analysis with optimal clusters\n",
    "plot_silhouette(sampled_df, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>While the distortion elbow suggested five clusters, silhouette analysis showed that a three-cluster solution provided significantly better cohesion and separation, with an average silhouette score of ~0.23. Even though 0.23 is weak, it is still considered acceptable for high-dimensional and real-world demographic data, where perfect separation is rare. The clusters are well-balanced in size, showed no signs of singled groups. Therefore, k=3 is most likely the ideal number of clusters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dimensionality Reduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply PCA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine k-clusterer with optimal number of clusters\n",
    "KClusterer = KMeans(n_clusters=3,\n",
    "                    verbose=0,\n",
    "                    random_state=2025)\n",
    "\n",
    "# Fit the model\n",
    "cluster_labels = KClusterer.fit_predict(sampled_df)\n",
    "\n",
    "# Apply PCA to scaled data\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(sampled_df)\n",
    "\n",
    "# Wrap into DataFrame for visualization/analysis\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "pca_df[\"Cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"Cluster\", palette=\"tab10\", s=10)\n",
    "plt.title(\"PCA Projection (PC1 vs PC2) Colored by Cluster\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top contributing features for each component\n",
    "components = pd.DataFrame(\n",
    "    pca.components_, \n",
    "    columns=scaled_df.columns, \n",
    "    index=[\"PC1\", \"PC2\", \"PC3\"]\n",
    ")\n",
    "\n",
    "for pc in components.index:\n",
    "    print(f\"\\n{pc} top features:\")\n",
    "    print(components.loc[pc].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "# Average PCA component values by cluster\n",
    "avg_components_by_cluster = pca_df.groupby(\"Cluster\")[[\"PC1\", \"PC2\", \"PC3\"]].mean().reset_index()\n",
    "print(\"\\nAverage component values by cluster:\")\n",
    "print(avg_components_by_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>UMAP</h3>\n",
    "<p>Now we will apply UMAP to reduce the dimensionality of the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP with commonly good starting values\n",
    "umap_model = umap.UMAP(n_neighbors=75,\n",
    "                    n_components=2,\n",
    "                    metric='cosine',\n",
    "                    n_epochs=None,\n",
    "                    min_dist=0.05,\n",
    "                    spread=1.0,\n",
    "                    low_memory=False,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=True\n",
    "                   )\n",
    "\n",
    "umap_2d = umap_model.fit_transform(sampled_df)\n",
    "\n",
    "# Create DataFrame\n",
    "umap_df = pd.DataFrame(umap_2d, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "umap_df[\"Cluster\"] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=umap_df, x=\"UMAP1\", y=\"UMAP2\", hue=\"Cluster\", palette=\"tab10\", s=10)\n",
    "plt.title(\"UMAP Projection Colored by Cluster\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>UMAP appears to perform worse than PCA. The clusters are less distinct</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2: Regression</h1>\n",
    "\n",
    "Now we will create models for a household’s proportion of income spent\n",
    "on total personal insurance premiums and retirement/pension contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regularized elastic net linear regression from your data.\n",
    "\n",
    "1. Create your target variable from the variables in the dataset. Do not use\n",
    "those components on the training database.\n",
    "2. Apply any data transformation / variable creation you deem necessary to\n",
    "obtain a good result.\n",
    "3. Discuss the grid that you chose to search for the parameters and the output\n",
    "that you obtained.\n",
    "4. For your test set, create a scatterplot of the original response and the\n",
    "predicted response. Report the MSE and R2 on the test set and calculate a\n",
    "bootstrapped confidence interval of the output.\n",
    "5. Interpret the coefficients of the top five most important variables in the\n",
    "regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting into Train and Test</h3>\n",
    "<p>We will split the data into a training and test set. We will use 30% of the data for training and 70% for testing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train, test = train_test_split(merged_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>\n",
    "<p>We will be using the same preprocessing steps as before, but considering the case that we now have train and test sets</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_processing_pipeline(train, test):\n",
    "    # Make sure Polars\n",
    "    if not isinstance(train, pl.DataFrame):\n",
    "        train = pl.from_pandas(train)\n",
    "    if not isinstance(test, pl.DataFrame):\n",
    "        test = pl.from_pandas(test)\n",
    "\n",
    "    # Create target variable before dropping anything\n",
    "    train = train.with_columns(\n",
    "        (pl.col(\"HSEP001S\") / pl.col(\"HSHNIAGG\")).alias(\"Target\")\n",
    "    )\n",
    "    test = test.with_columns(\n",
    "        (pl.col(\"HSEP001S\") / pl.col(\"HSHNIAGG\")).alias(\"Target\")\n",
    "    )\n",
    "\n",
    "    # === Preprocess TRAIN ===\n",
    "    train = convert_na_to_nulls(train)\n",
    "    train = convert_strings_to_numbers(train)\n",
    "    train = remove_rows_with_nulls_or_zeros(train)\n",
    "    train = substitute_nulls_with_median(train)\n",
    "    train = remove_negatives(train, [\"HSTT001\", \"HSTE001ZBS\"])\n",
    "    train = clean_up(train)\n",
    "    train = drop_related_target_columns(train)\n",
    "    train = train.drop([\"HSEP001S\", \"HSHNIAGG\"])\n",
    "\n",
    "    train, dropped_cols = remove_perfectly_correlated_columns(train)\n",
    "    train = apply_power_transformer(train)\n",
    "\n",
    "    # === Preprocess TEST ===\n",
    "    test = convert_na_to_nulls(test)\n",
    "    test = convert_strings_to_numbers(test)\n",
    "    test = remove_rows_with_nulls_or_zeros(test)\n",
    "    test = substitute_nulls_with_median(test)\n",
    "    test = remove_negatives(test, [\"HSTT001\", \"HSTE001ZBS\"])\n",
    "    test = clean_up(test)\n",
    "    test = drop_related_target_columns(test)\n",
    "    test = test.drop([\"HSEP001S\", \"HSHNIAGG\"])\n",
    "    test = test.drop([col for col in dropped_cols if col in test.columns])\n",
    "    test = apply_power_transformer(test)\n",
    "\n",
    "    # === Separate target ===\n",
    "    y_train = train[\"Target\"].to_pandas().reset_index(drop=True)\n",
    "    X_train = train.drop(\"Target\")\n",
    "    y_test = test[\"Target\"].to_pandas().reset_index(drop=True)\n",
    "    X_test = test.drop(\"Target\")\n",
    "\n",
    "    # === Scale features ===\n",
    "    scaler = RobustScaler()\n",
    "    column_names = X_train.columns\n",
    "\n",
    "    X_train = X_train.to_pandas()\n",
    "    X_test = X_test.to_pandas()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train_scaled, columns=column_names)\n",
    "    X_test = pd.DataFrame(X_test_scaled, columns=column_names)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = full_processing_pipeline(train, test)\n",
    "\n",
    "\n",
    "# Check if the shapes of X_train and X_test are the same\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Check if the shapes of y_train and y_test are the same\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Sample 20% of the training data\n",
    "sample_indices = X_train.sample(frac=0.20, random_state=2025).index\n",
    "\n",
    "# Apply the same index to both X and y\n",
    "sampled_X_train = X_train.loc[sample_indices]\n",
    "sampled_y_train = y_train.loc[sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Elastic Net</h2>\n",
    "<h3>Grid Search</h3>\n",
    "<p>We will be using a grid search to find the optimal parameters for the elastic net model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up grid search for alpha and l1_ratio\n",
    "param_grid = {\n",
    "    'alphas': [0.001, 0.005, 0.01],\n",
    "    'l1_ratio': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize the ElasticNetCV model\n",
    "elastic_net = ElasticNetCV(\n",
    "    alphas=param_grid['alphas'],\n",
    "    l1_ratio=param_grid['l1_ratio'],\n",
    "    cv=5,\n",
    "    max_iter=20000,\n",
    "    tol=1e-2,\n",
    "    n_jobs=-1,\n",
    "    random_state=2025\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "elastic_net.fit(sampled_X_train, sampled_y_train)\n",
    "\n",
    "print(f\"Best alpha: {elastic_net.alpha_}\")\n",
    "print(f\"Best l1_ratio: {elastic_net.l1_ratio_}\")\n",
    "\n",
    "final_model = ElasticNet(\n",
    "    alpha=elastic_net.alpha_,\n",
    "    l1_ratio=elastic_net.l1_ratio_,\n",
    "    max_iter=20000,\n",
    "    random_state=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Evaluation</h3>\n",
    "<p>We will be using the mean squared error and R2 score to evaluate the model. We will also be using a bootstrapped confidence interval to evaluate the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final model on the sampled training data\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Set MSE: {mse}\")\n",
    "print(f\"Test Set R²: {r2}\")\n",
    "\n",
    "# Plot: Actual vs Predicted\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, color=\"steelblue\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Household Proportion of Income Spent on Insurance & Retirement\")\n",
    "plt.ylabel(\"Predicted Household Proportion of Income Spent on Insurance & Retirement\")\n",
    "plt.title(\"Actual vs Predicted Values (Elastic Net)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bootstrapped CI\n",
    "n_bootstraps = 1000\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "y_test_array = np.array(y_test)\n",
    "y_pred_array = np.array(y_pred)\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test_array), len(y_test_array), replace=True)\n",
    "    mse_scores.append(mean_squared_error(y_test_array[indices], y_pred_array[indices]))\n",
    "    r2_scores.append(r2_score(y_test_array[indices], y_pred_array[indices]))\n",
    "\n",
    "mse_ci = np.percentile(mse_scores, [2.5, 97.5])\n",
    "r2_ci = np.percentile(r2_scores, [2.5, 97.5])\n",
    "\n",
    "print(f\"95% CI for MSE: [{mse_ci[0]}, {mse_ci[1]}]\")\n",
    "print(f\"95% CI for R²: [{r2_ci[0]}, {r2_ci[1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get and Interpret Top 5 Coefficients</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and coefficients from your fitted model\n",
    "feature_names = X_train.columns\n",
    "coefficients = final_model.coef_\n",
    "\n",
    "# Create a DataFrame to pair them\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Get the top 5 by absolute magnitude\n",
    "top5 = coef_df.reindex(coef_df.Coefficient.abs().sort_values(ascending=False).index).head(5)\n",
    "\n",
    "# Add interpretation column\n",
    "top5['Interpretation'] = top5['Coefficient'].apply(lambda x: \n",
    "    'Positive influence (increases target)' if x > 0 else \n",
    "    'Negative influence (decreases target)' if x < 0 else \n",
    "    'No effect'\n",
    ")\n",
    "\n",
    "top5.reset_index(drop=True, inplace=True)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost</h2>\n",
    "<p>We will be using the XGBoost model to predict the target variable. We will be using the same preprocessing steps as before.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = sampled_X_train.to_numpy()\n",
    "y_train_np = sampled_y_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grid Search</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 400],\n",
    "    'max_depth': [7, 9],\n",
    "    'learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', tree_method='hist', random_state=2025)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "grid_search.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model on the full training data\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"Test MSE:\", mse_xgb)\n",
    "print(\"Test R²:\", r2_xgb)\n",
    "\n",
    "# Scatterplot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.5, s=10, color=\"steelblue\")\n",
    "plt.plot([0, 0.15], [0, 0.15], 'r--')\n",
    "plt.xlabel(\"Actual Household Proportion of Income Spent on Insurance & Retirement\")\n",
    "plt.ylabel(\"Predicted Household Proportion of Income Spent on Insurance & Retirement\")\n",
    "plt.title(\"Actual vs Predicted (XGBoost)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_bootstraps = 1000\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    idx = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_true_sample = y_test[idx]\n",
    "    y_pred_sample = y_pred_xgb[idx]\n",
    "    \n",
    "    mse_scores.append(mean_squared_error(y_true_sample, y_pred_sample))\n",
    "    r2_scores.append(r2_score(y_true_sample, y_pred_sample))\n",
    "\n",
    "# Confidence intervals\n",
    "mse_ci = np.percentile(mse_scores, [2.5, 97.5])\n",
    "r2_ci = np.percentile(r2_scores, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap MSE CI:\", mse_ci)\n",
    "print(\"Bootstrap R² CI:\", r2_ci)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Shap Explainer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeExplainer and calculate SHAP values\n",
    "explainer = shap.Explainer(best_xgb)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot summary\n",
    "shap.summary_plot(shap_values, X_test, max_display=5)\n",
    "\n",
    "# Get mean absolute SHAP values\n",
    "mean_abs_shap = shap_values.abs.mean(0)\n",
    "top_5_idx = mean_abs_shap.values.argsort()[-5:][::-1]\n",
    "top_5_names = X_test.columns[top_5_idx]\n",
    "\n",
    "print(\"Top 5 SHAP Features & Mean SHAP Values:\")\n",
    "for i in range(5):\n",
    "    value = mean_abs_shap.values[top_5_idx[i]]  # extract raw float\n",
    "    print(f\"{top_5_names[i]}: {value:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
