{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn.feature_extraction.text as sktext\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 737)\n",
      "┌────────────┬────────┬────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
      "│ statistic  ┆ CODE   ┆ GEO    ┆ ECYASQKM ┆ … ┆ ECYTCA_18P ┆ ECYNCANCIT ┆ ECYNCA_U18 ┆ ECYNCA_18P │\n",
      "│ ---        ┆ ---    ┆ ---    ┆ ---      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
      "│ str        ┆ str    ┆ str    ┆ f64      ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
      "╞════════════╪════════╪════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
      "│ count      ┆ 868970 ┆ 868970 ┆ 868970.0 ┆ … ┆ 868970.0   ┆ 868970.0   ┆ 868970.0   ┆ 868970.0   │\n",
      "│ null_count ┆ 0      ┆ 0      ┆ 0.0      ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
      "│ mean       ┆ null   ┆ null   ┆ 0.0      ┆ … ┆ 32.767185  ┆ 5.353084   ┆ 0.85628    ┆ 4.496803   │\n",
      "│ std        ┆ null   ┆ null   ┆ 0.0      ┆ … ┆ 130.449848 ┆ 17.893635  ┆ 3.211073   ┆ 15.020402  │\n",
      "│ min        ┆ A0A0A0 ┆ FSALDU ┆ 0.0      ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
      "│ 25%        ┆ null   ┆ null   ┆ 0.0      ┆ … ┆ 6.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
      "│ 50%        ┆ null   ┆ null   ┆ 0.0      ┆ … ┆ 16.0       ┆ 1.0        ┆ 0.0        ┆ 1.0        │\n",
      "│ 75%        ┆ null   ┆ null   ┆ 0.0      ┆ … ┆ 33.0       ┆ 4.0        ┆ 1.0        ┆ 4.0        │\n",
      "│ max        ┆ Y1A7A4 ┆ FSALDU ┆ 0.0      ┆ … ┆ 17068.0    ┆ 1333.0     ┆ 318.0      ┆ 1164.0     │\n",
      "└────────────┴────────┴────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘\n",
      "shape: (9, 247)\n",
      "┌────────────┬────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
      "│ statistic  ┆ CODE   ┆ GEO    ┆ HSBASHHD  ┆ … ┆ HSTR056    ┆ HSTR056A   ┆ HSTR056B   ┆ HSTR057    │\n",
      "│ ---        ┆ ---    ┆ ---    ┆ ---       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
      "│ str        ┆ str    ┆ str    ┆ f64       ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
      "╞════════════╪════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
      "│ count      ┆ 868970 ┆ 868970 ┆ 868970.0  ┆ … ┆ 868970.0   ┆ 868970.0   ┆ 868970.0   ┆ 868970.0   │\n",
      "│ null_count ┆ 0      ┆ 0      ┆ 0.0       ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
      "│ mean       ┆ null   ┆ null   ┆ 18.432629 ┆ … ┆ 1322.43430 ┆ 831.592595 ┆ 490.84171  ┆ 1135.62168 │\n",
      "│            ┆        ┆        ┆           ┆   ┆ 4          ┆            ┆            ┆ 1          │\n",
      "│ std        ┆ null   ┆ null   ┆ 69.379951 ┆ … ┆ 6638.79984 ┆ 5076.88588 ┆ 2097.64949 ┆ 4203.80102 │\n",
      "│            ┆        ┆        ┆           ┆   ┆ 8          ┆ 2          ┆ 3          ┆ 1          │\n",
      "│ min        ┆ A0A0A0 ┆ FSALDU ┆ 0.0       ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
      "│ 25%        ┆ null   ┆ null   ┆ 3.0       ┆ … ┆ 85.412388  ┆ 36.712766  ┆ 23.624462  ┆ 88.826063  │\n",
      "│ 50%        ┆ null   ┆ null   ┆ 8.0       ┆ … ┆ 343.285857 ┆ 161.941917 ┆ 117.546716 ┆ 354.380653 │\n",
      "│ 75%        ┆ null   ┆ null   ┆ 18.0      ┆ … ┆ 1023.24791 ┆ 544.115663 ┆ 420.149131 ┆ 1009.68429 │\n",
      "│            ┆        ┆        ┆           ┆   ┆ 6          ┆            ┆            ┆ 9          │\n",
      "│ max        ┆ Y1A7A4 ┆ FSALDU ┆ 7751.0    ┆ … ┆ 1.4912e6   ┆ 1.2078e6   ┆ 397905.142 ┆ 733300.179 │\n",
      "│            ┆        ┆        ┆           ┆   ┆            ┆            ┆ 758        ┆ 849        │\n",
      "└────────────┴────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "demo_stats_df = pl.read_csv(\"Coursework Data/DemoStats.csv\", null_values=None)\n",
    "household_spend_df = pl.read_csv(\"Coursework Data/HouseholdSpend.csv\", null_values=None)\n",
    "\n",
    "print(demo_stats_df.describe())\n",
    "print(household_spend_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Stats Null Count: {'ECYPTAMED': {'null_count': 87459, 'null_percent': 10.06}, 'ECYPMAMED': {'null_count': 96444, 'null_percent': 11.1}, 'ECYPFAMED': {'null_count': 131837, 'null_percent': 15.17}, 'ECYHTAMED': {'null_count': 92709, 'null_percent': 10.67}, 'ECYHMAMED': {'null_count': 101698, 'null_percent': 11.7}, 'ECYHFAMED': {'null_count': 137900, 'null_percent': 15.87}, 'ECYMTNMED': {'null_count': 92709, 'null_percent': 10.67}}\n",
      "Household Stats Null Count: {}\n"
     ]
    }
   ],
   "source": [
    "# Define a function to convert NA to None\n",
    "def convert_na_to_nulls(df):\n",
    "    return df.with_columns([\n",
    "        pl.col(col).replace(\"NA\", None).alias(col)\n",
    "        if df.schema[col] == pl.Utf8 else pl.col(col)\n",
    "        for col in df.columns\n",
    "    ])\n",
    "\n",
    "# Convert NA to None on both dataframes\n",
    "demo_stats_df = convert_na_to_nulls(demo_stats_df)\n",
    "household_spend_df = convert_na_to_nulls(household_spend_df)\n",
    "\n",
    "# Get total nulls for each column\n",
    "def count_na_strings(df):\n",
    "    total_rows = df.height\n",
    "    return {\n",
    "        col: {\n",
    "            \"null_count\": df[col].null_count(),\n",
    "            \"null_percent\": round((df[col].null_count() / total_rows) * 100, 2)\n",
    "        }\n",
    "        for col in df.columns\n",
    "        if df[col].null_count() > 0\n",
    "    }\n",
    "\n",
    "demo_stats_null_cols = count_na_strings(demo_stats_df)\n",
    "household_spend_null_cols = count_na_strings(household_spend_df)\n",
    "# Initial Null Count for each csv file\n",
    "print(\"Demo Stats Null Count:\", demo_stats_null_cols)\n",
    "print(\"Household Stats Null Count:\", household_spend_null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you look about at the null counts, you can see that all columns with null counts have more than 1% nulls,\n",
    "# which means we can drop them. \n",
    "demo_stats_df = demo_stats_df.drop(null_cols for null_cols in demo_stats_null_cols)\n",
    "\n",
    "# Drop irrelevant Identifiers\n",
    "demo_stats_df = demo_stats_df.drop([\"CODE\", \"GEO\"])\n",
    "household_spend_df = household_spend_df.drop([\"CODE\",\"GEO\"])\n",
    "\n",
    "#Impute Missing Values (median or mode depending on data type)\n",
    "# say why in our report due to null counts (all the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Stats Negative Values: {}\n",
      "Household Spend Negative Values: {'HSTT001': {'negative_count': 28, 'negative_percent': 0.0}, 'HSTE001ZBS': {'negative_count': 163557, 'negative_percent': 18.82}, 'HSWH040S': {'negative_count': 255858, 'negative_percent': 29.44}, 'HSWH041S': {'negative_count': 15436, 'negative_percent': 1.78}, 'HSWH042S': {'negative_count': 19205, 'negative_percent': 2.21}}\n"
     ]
    }
   ],
   "source": [
    "# Get total negative values for each column\n",
    "def count_negative_values(df):\n",
    "    return {\n",
    "        col: {\n",
    "            \"negative_count\": df[col].filter(df[col] < 0).len(),\n",
    "            \"negative_percent\": round((df[col].filter(df[col] < 0).len() / df.height) * 100, 2)\n",
    "        }\n",
    "        for col in df.columns\n",
    "        if df.schema[col] in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                              pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                              pl.Float32, pl.Float64]\n",
    "        and df[col].filter(df[col] < 0).len() > 0\n",
    "    }\n",
    "\n",
    "# Get total negative values for each dataframe\n",
    "demo_stats_negatives = count_negative_values(demo_stats_df)\n",
    "household_spend_negatives = count_negative_values(household_spend_df)\n",
    "\n",
    "# Initial Negative Count for each csv file\n",
    "print(\"Demo Stats Negative Values:\", demo_stats_negatives)\n",
    "print(\"Household Spend Negative Values:\", household_spend_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HSTT001 - should not be negative as it describes total amount of money spent (clarify)\n",
    "\n",
    "# HSTE001ZBS - Total non-current consumption,Household Expenditures (Category Summary),Dollars\n",
    "'''- spending on long-term goods/services — such as:\n",
    "\n",
    "- Durable goods (cars, appliances)\n",
    "\n",
    "- Real estate\n",
    "\n",
    "- Long-term services or investments\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# HSWH041S - Net purchase price of owned secondary residences,Household Expenditures (Category Summary),Dollars\n",
    "\n",
    "# HSWH042S - Net purchase price of other owned properties,Household Expenditures (Category Summary),Dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "\n",
    "\n",
    "#standardize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 728)\n",
      "┌───────────┬──────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ statistic ┆ ECYASQKM ┆ ECYALSQKM ┆ ECYBASPOP ┆ … ┆ ECYTCA_18 ┆ ECYNCANCI ┆ ECYNCA_U1 ┆ ECYNCA_18 │\n",
      "│ ---       ┆ ---      ┆ ---       ┆ ---       ┆   ┆ P         ┆ T         ┆ 8         ┆ P         │\n",
      "│ str       ┆ f64      ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│           ┆          ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
      "╞═══════════╪══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ count     ┆ 868970.0 ┆ 868970.0  ┆ 868970.0  ┆ … ┆ 868970.0  ┆ 868970.0  ┆ 868970.0  ┆ 868970.0  │\n",
      "│ null_coun ┆ 0.0      ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ t         ┆          ┆           ┆           ┆   ┆           ┆           ┆           ┆           │\n",
      "│ mean      ┆ 0.0      ┆ 0.0       ┆ 46.684199 ┆ … ┆ 32.767185 ┆ 5.353084  ┆ 0.85628   ┆ 4.496803  │\n",
      "│ std       ┆ 0.0      ┆ 0.0       ┆ 174.83787 ┆ … ┆ 130.44984 ┆ 17.893635 ┆ 3.211073  ┆ 15.020402 │\n",
      "│           ┆          ┆           ┆ 7         ┆   ┆ 8         ┆           ┆           ┆           │\n",
      "│ min       ┆ 0.0      ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ 25%       ┆ 0.0      ┆ 0.0       ┆ 8.0       ┆ … ┆ 6.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ 50%       ┆ 0.0      ┆ 0.0       ┆ 21.0      ┆ … ┆ 16.0      ┆ 1.0       ┆ 0.0       ┆ 1.0       │\n",
      "│ 75%       ┆ 0.0      ┆ 0.0       ┆ 47.0      ┆ … ┆ 33.0      ┆ 4.0       ┆ 1.0       ┆ 4.0       │\n",
      "│ max       ┆ 0.0      ┆ 0.0       ┆ 23507.0   ┆ … ┆ 17068.0   ┆ 1333.0    ┆ 318.0     ┆ 1164.0    │\n",
      "└───────────┴──────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "shape: (9, 245)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ statistic ┆ HSBASHHD  ┆ HSHNIAGG  ┆ HSAGDISPI ┆ … ┆ HSTR056   ┆ HSTR056A  ┆ HSTR056B  ┆ HSTR057  │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ N         ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ str       ┆ f64       ┆ f64       ┆ ---       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "│           ┆           ┆           ┆ f64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ count     ┆ 868970.0  ┆ 868970.0  ┆ 868970.0  ┆ … ┆ 868970.0  ┆ 868970.0  ┆ 868970.0  ┆ 868970.0 │\n",
      "│ null_coun ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
      "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ mean      ┆ 18.432629 ┆ 2.2534e6  ┆ 1.7170e6  ┆ … ┆ 1322.4343 ┆ 831.59259 ┆ 490.84171 ┆ 1135.621 │\n",
      "│           ┆           ┆           ┆           ┆   ┆ 04        ┆ 5         ┆           ┆ 681      │\n",
      "│ std       ┆ 69.379951 ┆ 8.2787e6  ┆ 6.3276e6  ┆ … ┆ 6638.7998 ┆ 5076.8858 ┆ 2097.6494 ┆ 4203.801 │\n",
      "│           ┆           ┆           ┆           ┆   ┆ 48        ┆ 82        ┆ 93        ┆ 021      │\n",
      "│ min       ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
      "│ 25%       ┆ 3.0       ┆ 267191.07 ┆ 214098.26 ┆ … ┆ 85.412388 ┆ 36.712766 ┆ 23.624462 ┆ 88.82606 │\n",
      "│           ┆           ┆ 3885      ┆ 9455      ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "│ 50%       ┆ 8.0       ┆ 912463.57 ┆ 715484.19 ┆ … ┆ 343.28585 ┆ 161.94191 ┆ 117.54671 ┆ 354.3806 │\n",
      "│           ┆           ┆ 5234      ┆ 7728      ┆   ┆ 7         ┆ 7         ┆ 6         ┆ 53       │\n",
      "│ 75%       ┆ 18.0      ┆ 2.3017e6  ┆ 1.7612e6  ┆ … ┆ 1023.2479 ┆ 544.11566 ┆ 420.14913 ┆ 1009.684 │\n",
      "│           ┆           ┆           ┆           ┆   ┆ 16        ┆ 3         ┆ 1         ┆ 299      │\n",
      "│ max       ┆ 7751.0    ┆ 1.1651e9  ┆ 8.8699e8  ┆ … ┆ 1.4912e6  ┆ 1.2078e6  ┆ 397905.14 ┆ 733300.1 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 2758      ┆ 79849    │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Redisplay descriptive statistics\n",
    "print(demo_stats_df.describe())\n",
    "print(household_spend_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1: Clustering and Dimensionality Reduction</h1>\n",
    "\n",
    "The first part of the coursework will focus\n",
    "on identifying the characteristics of Canadian households, excluding their pension\n",
    "behaviour. For this, do not include, in your clustering and dimensionality reduction models,\n",
    "the target of the regression model in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data using your knowledge of the course. For context, you will\n",
    "create in later questions a model that predicts a household’s proportion of income\n",
    "spent on total personal insurance premiums and retirement/pension contributions\n",
    "and apply a clustering algorithm over the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset, but separate target\n",
    "\n",
    "# Deal with null values, outliers, dirty values, inconsistent values (GIGO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a K-Means clustering of the data, identifying the optimal number of\n",
    "clusters using both the silhouette and the elbow method. Do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KClusterer\n",
    "\n",
    "# Use KElbowVisualizer to find optimal number of clusters\n",
    "\n",
    "# Use Silhouette to find optimal number of clusters\n",
    "\n",
    "# Compare results and determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply a linear dimensionality reductions technique to the data.\n",
    "1. Apply PCA to your data.\n",
    "2. Plot the data of the first two PCs in a scatterplot and colour the points as\n",
    "per the cluster labels you calculated in the previous step. What can you say\n",
    "about your data? Are your clusters clearly defined in the output? Interpret\n",
    "the first three components of your PCA output.\n",
    "3. For the first three components, calculate the average value of each\n",
    "component, for each cluster, so your data should look like a table with\n",
    "cluster number and average component value. Give a name to the clusters\n",
    "from this output and justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, use UMAP to reduce the data to two dimensions. Justify your choice of\n",
    "parameters by searching for the optimal value as you deem reasonable. Again,\n",
    "colour the data to differentiate each cluster that you named. What do you see? Is\n",
    "your UMAP a better or worse interpretation than PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2: Regression</h1>\n",
    "\n",
    "Now we will create models for a household’s proportion of income spent\n",
    "on total personal insurance premiums and retirement/pension contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regularized elastic net linear regression from your data.\n",
    "\n",
    "1. Create your target variable from the variables in the dataset. Do not use\n",
    "those components on the training database.\n",
    "2. Apply any data transformation / variable creation you deem necessary to\n",
    "obtain a good result.\n",
    "3. Discuss the grid that you chose to search for the parameters and the output\n",
    "that you obtained.\n",
    "4. For your test set, create a scatterplot of the original response and the\n",
    "predicted response. Report the MSE and R2 on the test set and calculate a\n",
    "bootstrapped confidence interval of the output.\n",
    "5. Interpret the coefficients of the top five most important variables in the\n",
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
